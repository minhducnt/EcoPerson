{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_node_data(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    return df\n",
    "\n",
    "csv_file_path = 'node_data.csv'\n",
    "df = read_node_data(csv_file_path)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "window_size = 50\n",
    "df['ph_trend'] = df.groupby('node_id')['ph'].rolling(window=window_size).mean().reset_index(level=0, drop=True)\n",
    "df['tds_trend'] = df.groupby('node_id')['tds'].rolling(window=window_size).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "def isolation_forest_scorer(estimator, X):\n",
    "    y_pred = estimator.predict(X)\n",
    "    y_true = np.ones(len(X))\n",
    "    return f1_score(y_true, y_pred == -1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_samples': ['auto', 0.5, 0.75],\n",
    "    'contamination': [0.05, 0.1, 0.15],\n",
    "    'max_features': [1.0, 0.75, 0.5]\n",
    "}\n",
    "\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=iso_forest,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring=make_scorer(isolation_forest_scorer),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(df[['ph', 'tds']])\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "optimized_clf = IsolationForest(\n",
    "    n_estimators=best_params['n_estimators'], \n",
    "    max_samples=best_params['max_samples'], \n",
    "    contamination=best_params['contamination'], \n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_clf.fit(df[['ph', 'tds']])\n",
    "\n",
    "with open('optimized_clf.pkl', 'wb') as model_file:\n",
    "    pickle.dump(optimized_clf, model_file)\n",
    "\n",
    "with open('optimized_clf.pkl', 'rb') as model_file:\n",
    "    loaded_clf = pickle.load(model_file)\n",
    "\n",
    "df['anomaly_optimized'] = loaded_clf.predict(df[['ph', 'tds']])\n",
    "\n",
    "anomalous_nodes_optimized = df[df['anomaly_optimized'] == -1]\n",
    "\n",
    "db = DBSCAN(eps=1.5, min_samples=10).fit(df[['ph', 'tds']])\n",
    "\n",
    "df['cluster_optimized'] = db.labels_\n",
    "\n",
    "affected_nodes_optimized = []\n",
    "core_nodes_optimized = anomalous_nodes_optimized['node_id'].unique()\n",
    "\n",
    "affected_by = {}\n",
    "\n",
    "for node_id in core_nodes_optimized:\n",
    "    node_cluster = df[df['node_id'] == node_id]['cluster_optimized'].iloc[0]\n",
    "    if node_cluster != -1:\n",
    "        affected_nodes = df[df['cluster_optimized'] == node_cluster]['node_id'].unique()\n",
    "        affected_nodes = [node for node in affected_nodes if node != node_id]\n",
    "        affected_nodes_optimized.extend(affected_nodes)\n",
    "        for affected_node in affected_nodes:\n",
    "            if affected_node not in affected_by:\n",
    "                affected_by[affected_node] = []\n",
    "            affected_by[affected_node].append(node_id)\n",
    "\n",
    "affected_nodes_optimized = list(set(affected_nodes_optimized))\n",
    "\n",
    "affected_nodes_optimized = [node for node in affected_nodes_optimized if node not in core_nodes_optimized]\n",
    "risk_threshold_ph = 8.5\n",
    "risk_threshold_tds = 400\n",
    "at_risk_nodes = df[(df['ph_trend'] > risk_threshold_ph) | (df['tds_trend'] > risk_threshold_tds)]\n",
    "at_risk_node_ids = at_risk_nodes['node_id'].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(df[df['anomaly_optimized'] == 1]['ph'], df[df['anomaly_optimized'] == 1]['tds'], color='blue', label='Các node bình thường')\n",
    "plt.scatter(anomalous_nodes_optimized['ph'], anomalous_nodes_optimized['tds'], color='red', label='Các node bị ô nhiễm')\n",
    "plt.scatter(df[df['node_id'].isin(at_risk_node_ids)]['ph'], df[df['node_id'].isin(at_risk_node_ids)]['tds'], color='purple', label='Các node có nguy cơ ô nhiễm')\n",
    "plt.xlabel('pH')\n",
    "plt.ylabel('TDS')\n",
    "plt.title('Phân bố các node trong mạng IOT')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "result_optimized = []\n",
    "\n",
    "for node_id in df['node_id'].unique():\n",
    "    node_data = df[df['node_id'] == node_id]\n",
    "    node_type = 'normal'\n",
    "    extra_info = {}\n",
    "    if node_id in anomalous_nodes_optimized['node_id'].unique():\n",
    "        node_type = 'polluted'\n",
    "    elif node_id in affected_nodes_optimized:\n",
    "        node_type = 'effected'\n",
    "    elif node_id in at_risk_node_ids:\n",
    "        node_type = 'risk'\n",
    "        extra_info = {\n",
    "            'meanPh': node_data['ph_trend'].mean(),\n",
    "            'meanTDS': node_data['tds_trend'].mean()\n",
    "        }\n",
    "    result = {\n",
    "        'node_id': node_id,\n",
    "        'node_type': node_type,\n",
    "        'meanPh': node_data['ph'].mean(),\n",
    "        'meanTDS': node_data['tds'].mean()\n",
    "    }\n",
    "    result.update(extra_info)\n",
    "    result_optimized.append(result)\n",
    "\n",
    "result_optimized_df = pd.DataFrame(result_optimized)\n",
    "print(result_optimized_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
